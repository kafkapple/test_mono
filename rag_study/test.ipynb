{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install langchain langchain-community langchain-core faiss-cpu tiktoken upstage upstage-ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LangSmithClient' from 'langsmith' (c:\\Users\\joon\\miniconda3\\envs\\rag\\lib\\site-packages\\langsmith\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpassthrough\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunnablePassthrough\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangsmith\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LangSmithClient\n\u001b[0;32m     17\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LangSmithClient' from 'langsmith' (c:\\Users\\joon\\miniconda3\\envs\\rag\\lib\\site-packages\\langsmith\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.passthrough import RunnablePassthrough\n",
    "import tiktoken\n",
    "from langsmith import LangSmithClient\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"all you need is attention.pdf\"  # PDF ë˜ëŠ” TXT íŒŒì¼\n",
    "\n",
    "if file_path.endswith(\".pdf\"):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "elif file_path.endswith(\".txt\"):\n",
    "    loader = TextLoader(file_path)\n",
    "else:\n",
    "    raise ValueError(\"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹\")\n",
    "\n",
    "documents = loader.load()\n",
    "logging.info(f\"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(documents)} ê°œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter_method = \"recursive\"  # \"recursive\" ë˜ëŠ” \"token\"\n",
    "\n",
    "if splitter_method == \"recursive\":\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "elif splitter_method == \"token\":\n",
    "    splitter = TokenTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "else:\n",
    "    raise ValueError(\"ì§€ì›ë˜ì§€ ì•ŠëŠ” splitter ë°©ì‹\")\n",
    "\n",
    "split_docs = splitter.split_documents(documents)\n",
    "logging.info(f\"âœ… ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: {len(split_docs)} ê°œ ì²­í¬ ìƒì„±\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_method = \"openai\"  # \"openai\", \"bge\", \"upstage\"\n",
    "\n",
    "if embedding_method == \"openai\":\n",
    "    embedding_model = OpenAIEmbeddings()\n",
    "elif embedding_method == \"bge\":\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en\")\n",
    "elif embedding_method == \"upstage\":\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"Upstage/Solar-embedding\")\n",
    "else:\n",
    "    raise ValueError(\"ì§€ì›ë˜ì§€ ì•ŠëŠ” Embedding ë°©ì‹\")\n",
    "\n",
    "logging.info(f\"âœ… ì„ë² ë”© ëª¨ë¸ ì„ íƒ: {embedding_method.upper()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = FAISS.from_documents(split_docs, embedding_model)\n",
    "logging.info(f\"âœ… ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})  \n",
    "# search_type: \"similarity\", \"mmr\" (ìµœëŒ€ ë§ˆì§„ ê²€ìƒ‰)\n",
    "# k: ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜\n",
    "\n",
    "logging.info(f\"âœ… ê²€ìƒ‰ê¸° ì„¤ì • ì™„ë£Œ (Top-K: {retriever.search_kwargs['k']}, ë°©ì‹: {retriever.search_type})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"openai\"  # \"openai\", \"upstage\", \"lmstudio\"\n",
    "\n",
    "if llm_model == \"openai\":\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4\")\n",
    "elif llm_model == \"upstage\":\n",
    "    llm = ChatOpenAI(model_name=\"solar-10.7b\")\n",
    "elif llm_model == \"lmstudio\":\n",
    "    llm = ChatOpenAI(base_url=\"http://localhost:1234/v1\", model_name=\"lmstudio\")\n",
    "else:\n",
    "    raise ValueError(\"ì§€ì›ë˜ì§€ ì•ŠëŠ” LLM ëª¨ë¸\")\n",
    "\n",
    "logging.info(f\"âœ… LLM ëª¨ë¸ ì„ íƒ: {llm_model.upper()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„±\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever.search_and_merge,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "logging.info(\"âœ… ê²€ìƒ‰ ì²´ì¸ ìƒì„± ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenMonitor:\n",
    "    \"\"\"LLM ì‘ë‹µì˜ í† í° ê¸¸ì´ë¥¼ ì¶”ì í•˜ê³  LangSmithì— ì €ì¥\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = LangSmithClient(api_key=\"your-langsmith-api-key\")\n",
    "\n",
    "    def track(self, response):\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        token_count = len(enc.encode(response))\n",
    "        logging.info(f\"âœ… ìƒì„±ëœ ì‘ë‹µ í† í° ìˆ˜: {token_count}\")\n",
    "        self.client.log_event(\"response_token\", {\"token_count\": token_count})\n",
    "\n",
    "token_monitor = TokenMonitor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"LangChainì´ë€?\"\n",
    "response = chain.invoke({\"question\": query})\n",
    "token_monitor.track(response)\n",
    "\n",
    "print(\"ğŸ”¹ AI ì‘ë‹µ:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
